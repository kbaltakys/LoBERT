{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from datasets import Dataset\n",
    "from transformers import BertForMaskedLM\n",
    "from theorder import LoBertModel, LoBertConfig, LoBertForMaskedLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'time_ids', 'volume_ids'],\n",
       "    num_rows: 3040\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_data_sample = \"/workspaces/2025 LoBERT/data/LOBSTER_SampleFile_AAPL_2012-06-21_10/ArrowDataset\"\n",
    "ds = Dataset.load_from_disk(path_data_sample)\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetInfo(description='', citation='', homepage='', license='', features={'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'time_ids': Sequence(feature=Value(dtype='float64', id=None), length=-1, id=None), 'volume_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}, post_processed=None, supervised_keys=None, builder_name=None, dataset_name=None, config_name=None, version=None, splits=None, download_checksums=None, download_size=None, post_processing_size=None, dataset_size=None, size_in_bytes=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoBertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 128,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 512,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"lobert\",\n",
       "  \"num_attention_heads\": 2,\n",
       "  \"num_hidden_layers\": 2,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.48.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 3200\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_lobert_tiny = {\n",
    "    \"hidden_size\": 128,\n",
    "    \"intermediate_size\": 512,\n",
    "    \"layer_norm_eps\": 1e-12,\n",
    "    \"max_position_embeddings\": 512,\n",
    "    \"model_type\": \"lobert\",\n",
    "    \"num_attention_heads\": 2,\n",
    "    \"num_hidden_layers\": 2,\n",
    "    \"vocab_size\": 3200\n",
    "}\n",
    "config = LoBertConfig(**config_lobert_tiny)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.bos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base LoBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoBERT model has 889,216 parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LoBertModel(\n",
       "  (embeddings): LoBertEmbeddings(\n",
       "    (message_embeddings): Embedding(3200, 128, padding_idx=0)\n",
       "    (time_embeddings): ValueProjectionEmbedding(\n",
       "      (value_projection): Linear(in_features=1, out_features=128, bias=True)\n",
       "    )\n",
       "    (volume_embeddings): ValueProjectionEmbedding(\n",
       "      (value_projection): Linear(in_features=1, out_features=128, bias=True)\n",
       "    )\n",
       "    (position_embeddings): Embedding(512, 128)\n",
       "    (token_type_embeddings): Embedding(2, 128)\n",
       "    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-1): 2 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = LoBertModel(config)\n",
    "print(f\"LoBERT model has {base_model.num_parameters():,.0f} parameters\")\n",
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = ds.with_format(\"torch\")[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.7275, -0.1028, -0.1403,  ..., -1.9343, -0.2860, -0.3965],\n",
       "         [ 1.1002,  2.4706,  0.7439,  ..., -2.0784,  0.0186, -1.0983],\n",
       "         [ 1.3703,  3.0149,  1.0603,  ..., -2.1795,  0.1434, -1.2215],\n",
       "         ...,\n",
       "         [-0.3715, -0.8666, -1.3562,  ..., -0.5026, -0.7140,  0.7688],\n",
       "         [-0.2333, -0.8788, -1.1629,  ..., -0.4724, -0.5513,  0.6603],\n",
       "         [-0.1844, -0.8188, -1.2360,  ..., -0.4445, -0.5595,  0.7353]],\n",
       "\n",
       "        [[-0.1977, -0.6423, -1.1199,  ..., -0.2547, -0.7114,  0.7132],\n",
       "         [-0.1733,  0.9481,  0.0073,  ...,  0.8994, -1.1154,  0.8086],\n",
       "         [ 0.1012, -0.8389,  0.1236,  ..., -0.5018, -0.6408,  0.7601],\n",
       "         ...,\n",
       "         [ 0.0247,  1.5670,  0.1220,  ..., -1.9427, -0.4349, -0.4982],\n",
       "         [ 1.2271,  2.6801,  0.9897,  ..., -2.0800,  0.0821, -1.1131],\n",
       "         [ 1.0642,  2.4116,  0.5120,  ..., -2.0707, -0.0940, -1.0699]],\n",
       "\n",
       "        [[ 0.0841,  2.6060,  0.9784,  ..., -1.9265,  0.0562, -0.9615],\n",
       "         [ 1.1171,  2.6752,  0.9344,  ..., -2.0683, -0.1536,  0.0130],\n",
       "         [-0.2381, -0.8685, -1.1829,  ..., -0.3212, -0.0697,  0.7120],\n",
       "         ...,\n",
       "         [ 1.1005,  2.3963,  0.6654,  ..., -2.0635, -0.0279, -0.0796],\n",
       "         [ 1.1447,  2.5108,  0.0214,  ..., -2.0375,  0.0096, -0.0467],\n",
       "         [ 1.2450,  2.8257,  0.8985,  ..., -2.0505,  0.0431, -1.0736]],\n",
       "\n",
       "        [[-0.0522, -0.1627, -1.0654,  ..., -0.9027, -0.5582,  0.3600],\n",
       "         [ 0.7876,  1.7799, -0.0543,  ...,  0.0394, -0.3315, -0.6372],\n",
       "         [-0.0299,  0.0960, -0.9676,  ..., -1.0399, -0.5896,  0.1676],\n",
       "         ...,\n",
       "         [ 0.0648,  2.6384,  0.9793,  ..., -2.1154,  0.0368, -1.2579],\n",
       "         [ 1.1508,  2.6579,  0.0550,  ..., -2.0169, -0.0056, -1.1294],\n",
       "         [ 0.0198,  2.6619,  0.9783,  ..., -2.0357,  0.0338, -1.1182]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-2.2304e-01,  3.4652e-01, -2.4721e-01,  1.4366e-01,  1.5208e-01,\n",
       "         -3.4193e-01,  1.3909e-01,  2.3542e-02, -2.0408e-02, -2.8574e-01,\n",
       "          2.2624e-02, -5.0302e-02,  7.0639e-03,  6.1403e-02,  1.3835e-01,\n",
       "         -1.3149e-01,  3.4024e-01, -2.0701e-01, -1.7148e-01,  2.2505e-01,\n",
       "          1.5864e-01, -3.7161e-01,  4.0088e-01, -2.0230e-01,  2.5452e-01,\n",
       "          3.0158e-01, -4.9480e-03, -2.2598e-01, -5.9761e-02, -3.2835e-01,\n",
       "          3.3838e-01, -8.3897e-02,  2.5909e-01,  1.2846e-03, -1.7109e-01,\n",
       "         -1.9452e-01, -1.8884e-01, -3.3063e-02, -2.0239e-01, -1.5674e-01,\n",
       "         -6.3609e-02,  6.9979e-02,  2.6184e-01, -1.3312e-01,  2.0276e-01,\n",
       "          2.3047e-01,  2.9387e-01,  1.1536e-01, -1.6371e-02,  9.8678e-02,\n",
       "          2.4751e-01,  1.2293e-01, -1.0151e-01, -1.8713e-01, -5.1027e-02,\n",
       "          1.6984e-01,  4.1189e-02,  2.0024e-01, -1.1676e-01, -2.9725e-02,\n",
       "         -3.2167e-01, -1.7658e-01,  1.2771e-02,  2.3538e-01,  1.0346e-02,\n",
       "          4.5288e-01,  5.2878e-01,  2.9629e-02,  4.1431e-02,  2.2648e-01,\n",
       "         -1.1434e-01, -2.3067e-01,  2.2663e-01,  2.8858e-01,  1.1680e-01,\n",
       "          1.9325e-01, -4.0606e-01,  2.4115e-02,  2.8958e-01, -5.9466e-01,\n",
       "          2.4128e-01,  3.0916e-01,  1.6306e-01, -7.6839e-02, -1.3067e-01,\n",
       "          2.2645e-01, -1.2704e-01, -2.1082e-01, -1.3353e-01,  2.0200e-01,\n",
       "         -1.7030e-01,  1.8069e-01,  7.9483e-03,  1.4770e-01,  8.7539e-02,\n",
       "         -8.2483e-02,  4.7912e-01, -1.5021e-01, -1.5640e-01,  1.5290e-01,\n",
       "         -1.3230e-01,  4.0258e-01, -2.2127e-01,  3.8960e-01,  2.0725e-01,\n",
       "          3.4894e-01,  5.2713e-02,  1.3167e-01, -1.0913e-01,  4.1894e-01,\n",
       "          4.1512e-01,  2.0631e-02,  1.1067e-01, -4.2753e-02,  1.6166e-01,\n",
       "          1.2021e-01,  3.6367e-01,  1.5647e-02,  1.0815e-02,  1.4197e-01,\n",
       "          3.0363e-01, -9.3680e-02, -1.7696e-01, -1.9349e-01,  9.0914e-02,\n",
       "          3.9703e-02, -5.4098e-02,  2.4540e-01],\n",
       "        [-4.2090e-01,  1.2305e-01, -3.6446e-01,  2.2758e-01,  3.5207e-02,\n",
       "         -6.6624e-02,  4.7305e-02,  3.0521e-01,  3.1303e-01, -2.1579e-01,\n",
       "         -6.7576e-02, -1.9539e-02,  1.8871e-03, -1.2969e-01,  7.2719e-03,\n",
       "         -1.9870e-01,  5.5108e-02, -1.2812e-01, -3.2543e-01,  2.6168e-01,\n",
       "          2.9267e-01, -6.1381e-02,  3.6913e-01, -2.6046e-02,  6.1935e-02,\n",
       "          2.3006e-01, -1.4257e-01, -1.3285e-02, -9.1093e-02, -1.6984e-01,\n",
       "          4.4051e-01, -1.5516e-01,  3.7668e-01, -5.6246e-05, -3.6337e-01,\n",
       "         -9.8314e-02, -2.8054e-01, -8.8816e-02, -4.5617e-01,  2.2121e-01,\n",
       "         -1.8965e-01, -8.6694e-02,  1.6609e-01,  4.6681e-03,  2.4159e-02,\n",
       "          1.9628e-01,  1.8326e-01,  2.2787e-02, -1.6512e-01,  1.6873e-02,\n",
       "          3.1227e-02,  5.2255e-01, -1.9957e-01,  1.7744e-01, -1.1459e-01,\n",
       "          1.7098e-01, -1.0217e-01, -1.8928e-01,  9.5128e-02, -5.6138e-01,\n",
       "         -3.8512e-01, -1.8817e-01, -1.9206e-01,  4.6771e-01, -7.9528e-03,\n",
       "          2.0073e-01,  2.6906e-01, -1.4445e-01, -7.2853e-02, -2.3202e-02,\n",
       "          3.4694e-01, -9.8452e-02, -9.1099e-02, -2.3464e-01,  3.3424e-01,\n",
       "         -6.4967e-02, -1.8297e-01,  1.5817e-01,  3.8812e-01, -5.6067e-01,\n",
       "         -5.1940e-02,  3.7870e-01,  1.0691e-01, -2.3658e-01, -6.4327e-02,\n",
       "         -6.0691e-02,  4.9080e-02,  1.6629e-01, -2.7126e-01,  1.9270e-01,\n",
       "         -1.8460e-01,  2.5506e-01,  1.2458e-02,  4.8990e-01,  8.1025e-02,\n",
       "          1.0670e-01, -1.7346e-01, -4.6819e-03,  1.6434e-01,  7.2938e-02,\n",
       "         -8.1157e-02, -2.1603e-02, -2.3886e-01,  4.2474e-01,  2.9881e-01,\n",
       "          1.9205e-01, -7.7555e-03,  1.1119e-01, -1.4760e-01,  1.5083e-01,\n",
       "          3.5104e-02, -1.9149e-01,  6.8884e-02, -9.0309e-02, -8.1822e-02,\n",
       "          5.6162e-02,  2.7371e-01,  9.4286e-02, -8.6994e-02,  1.4360e-01,\n",
       "          2.3118e-02, -1.2777e-01,  1.9582e-03, -1.1903e-01,  6.2428e-02,\n",
       "         -3.3296e-01, -3.0888e-01,  4.8117e-02],\n",
       "        [-2.4336e-01,  1.7303e-01, -5.1400e-02, -1.5319e-01,  2.9364e-01,\n",
       "         -2.7244e-01,  1.0874e-01, -2.2837e-01, -2.3491e-01, -1.1907e-01,\n",
       "          3.3320e-02, -2.2915e-01,  1.0504e-01,  7.6660e-02,  3.8740e-01,\n",
       "          1.4530e-01,  2.4715e-01, -2.2971e-01, -2.0890e-02,  1.2716e-01,\n",
       "         -1.8909e-02, -3.7156e-01,  2.7672e-01, -2.2708e-01,  1.5810e-01,\n",
       "          3.2244e-01, -2.8886e-01, -3.1178e-01, -1.9019e-02, -1.9205e-01,\n",
       "         -1.8010e-01,  3.1622e-03,  5.8412e-02, -5.8430e-02,  2.2185e-01,\n",
       "         -2.4253e-01,  9.0130e-02,  8.8086e-02,  1.7912e-02, -3.1676e-02,\n",
       "          1.8168e-01,  4.6311e-02,  2.1574e-01, -2.0058e-01,  2.3897e-01,\n",
       "          1.5547e-01,  1.5683e-01,  1.1497e-01, -7.0502e-02, -9.4867e-02,\n",
       "          1.3410e-01, -3.5087e-02, -2.4484e-01, -3.3279e-01, -1.2304e-01,\n",
       "          2.5839e-02,  1.4352e-01,  3.3107e-01, -9.2692e-02,  5.5497e-01,\n",
       "          2.1603e-01,  8.9733e-02,  1.1015e-01, -4.0541e-01, -1.4956e-01,\n",
       "          2.7855e-01,  2.3532e-01,  1.6158e-01, -1.7140e-01,  4.2681e-01,\n",
       "         -3.9687e-01, -1.9299e-01,  4.6632e-01,  3.7506e-01,  2.4914e-02,\n",
       "          2.1984e-01, -1.9002e-01,  1.1236e-02, -5.8439e-02, -3.5922e-01,\n",
       "          4.6276e-01,  6.1576e-02,  2.3206e-01, -1.2497e-01, -9.3686e-02,\n",
       "          2.9983e-01, -4.7417e-02, -3.7967e-01, -4.6454e-02,  3.5342e-01,\n",
       "         -1.3995e-01,  4.2210e-03,  1.0411e-01, -2.3391e-01,  1.2335e-01,\n",
       "         -1.7137e-01,  4.9413e-01, -2.3472e-02, -3.0960e-01,  6.4630e-02,\n",
       "          1.4580e-01,  4.6459e-01,  9.3798e-02,  8.3955e-02,  9.7039e-02,\n",
       "          1.9868e-01,  3.4755e-02,  4.5011e-02, -1.0392e-01,  5.1788e-01,\n",
       "          2.3495e-01,  1.7981e-01, -1.5245e-01,  2.2127e-01,  1.5505e-01,\n",
       "         -1.4454e-01,  1.1358e-02, -1.8603e-01,  5.9457e-02, -7.0772e-02,\n",
       "          2.6749e-01,  1.3763e-02, -9.8696e-02, -2.6457e-01,  1.7244e-01,\n",
       "          2.3428e-01,  1.9899e-01,  1.8809e-01],\n",
       "        [-3.4357e-01,  2.8716e-01, -3.3886e-01,  1.0448e-01,  1.4938e-02,\n",
       "         -1.9524e-01, -1.1591e-01,  1.7142e-01,  2.7969e-01, -1.5592e-01,\n",
       "          9.5774e-02, -1.8666e-02, -6.3578e-02, -8.6876e-02,  4.8930e-02,\n",
       "         -3.1598e-01,  1.3966e-01, -2.5896e-01, -1.1207e-01,  2.7915e-01,\n",
       "          2.3403e-01, -1.0294e-01,  4.3855e-01, -7.2378e-02,  1.2773e-01,\n",
       "          3.0535e-01, -1.2140e-01, -1.5954e-02, -4.9249e-02, -6.7880e-02,\n",
       "          3.2912e-01, -2.1436e-01,  2.6534e-01,  1.7204e-01, -3.6329e-01,\n",
       "         -2.1251e-01, -2.5485e-01, -1.1990e-02, -2.6024e-01,  1.4122e-01,\n",
       "         -2.3645e-01, -2.1048e-02,  3.0457e-01,  2.9755e-02,  4.2857e-02,\n",
       "          2.3670e-01,  1.2762e-01, -6.0645e-02, -7.7450e-02,  2.8973e-01,\n",
       "          1.6486e-01,  3.5984e-01, -1.1163e-02,  1.4756e-01, -1.3946e-01,\n",
       "          7.2303e-02, -1.0057e-01, -9.6579e-02,  1.9003e-02, -3.4572e-01,\n",
       "         -3.0564e-01, -2.7283e-01, -1.4761e-01,  4.6275e-01,  7.4935e-02,\n",
       "          3.4335e-01,  4.5372e-01, -2.9563e-02,  1.6242e-01,  1.2160e-01,\n",
       "          2.5997e-01, -9.4694e-02,  3.4345e-02, -2.3272e-01,  1.3758e-01,\n",
       "         -8.2620e-02, -1.6548e-01,  1.2547e-02,  2.9963e-01, -5.7114e-01,\n",
       "          4.1454e-02,  3.4260e-01,  1.3742e-01, -1.2008e-01, -9.3070e-02,\n",
       "         -5.4155e-03, -4.7630e-02,  1.6078e-03, -2.7748e-01,  4.0364e-02,\n",
       "         -3.7205e-02,  2.9528e-01,  9.6807e-02,  3.8082e-01,  8.1313e-02,\n",
       "          2.6493e-01,  1.5863e-01, -6.0959e-02,  6.2266e-02,  6.0268e-02,\n",
       "         -1.4791e-01,  1.0039e-01, -2.9628e-01,  4.1273e-01,  2.1410e-01,\n",
       "          3.6398e-01, -1.5758e-01,  1.3470e-01, -5.4235e-02,  1.5614e-01,\n",
       "          2.3217e-01,  3.7506e-02, -3.5036e-02, -1.1525e-01,  6.9955e-02,\n",
       "          2.4572e-02,  3.7163e-01,  9.0676e-02, -1.6898e-01,  6.4885e-02,\n",
       "          2.8841e-01, -2.2380e-01, -2.8765e-03, -2.1474e-01,  1.9964e-01,\n",
       "         -1.1028e-01, -1.6463e-01,  4.7302e-02]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model(**input_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked message modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LoBertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LoBertForMaskedLM(\n",
       "  (bert): LoBertModel(\n",
       "    (embeddings): LoBertEmbeddings(\n",
       "      (message_embeddings): Embedding(3200, 128, padding_idx=0)\n",
       "      (time_embeddings): ValueProjectionEmbedding(\n",
       "        (value_projection): Linear(in_features=1, out_features=128, bias=True)\n",
       "      )\n",
       "      (volume_embeddings): ValueProjectionEmbedding(\n",
       "        (value_projection): Linear(in_features=1, out_features=128, bias=True)\n",
       "      )\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-1): 2 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): LoBertOnlyMLMHead(\n",
       "    (predictions): LoBertLMPredictionHead(\n",
       "      (transform): LoBertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=128, out_features=3200, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmmodel = LoBertForMaskedLM(config)\n",
    "mmmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from theorder import DataCollatorForMessageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class LoBertTokenizer:\n",
    "    mask_token: str = \"[MASK]\"\n",
    "\n",
    "tokenizer = LoBertTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForMessageModeling(tokenizer=tokenizer, mlm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'time_ids', 'volume_ids'],\n",
       "    num_rows: 3040\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LoBertTokenizer' object has no attribute 'pad'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mdata_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/lob001/lib/python3.11/site-packages/transformers/data/data_collator.py:45\u001b[0m, in \u001b[0;36mDataCollatorMixin.__call__\u001b[0;34m(self, features, return_tensors)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtf_call(features)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy_call(features)\n",
      "File \u001b[0;32m/workspaces/2025 LoBERT/src/package/theorder/data/data_collator.py:62\u001b[0m, in \u001b[0;36mDataCollatorForMessageModeling.torch_call\u001b[0;34m(self, examples)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtorch_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, examples: List[Union[List[\u001b[38;5;28mint\u001b[39m], Any, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# Handle dict or lists with proper padding and conversion to tensor.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(examples[\u001b[38;5;241m0\u001b[39m], Mapping):\n\u001b[0;32m---> 62\u001b[0m         batch \u001b[38;5;241m=\u001b[39m \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     66\u001b[0m         batch \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     67\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: _torch_collate_batch(examples, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer, pad_to_multiple_of\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_to_multiple_of)\n\u001b[1;32m     68\u001b[0m         }\n",
      "File \u001b[0;32m/opt/conda/envs/lob001/lib/python3.11/site-packages/transformers/data/data_collator.py:59\u001b[0m, in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[0;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# To avoid errors when using Feature extractors\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tokenizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecation_warnings\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m(\u001b[38;5;241m*\u001b[39mpad_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpad_kwargs)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Save the state of the warning, then disable it\u001b[39;00m\n\u001b[1;32m     62\u001b[0m warning_state \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LoBertTokenizer' object has no attribute 'pad'"
     ]
    }
   ],
   "source": [
    "out = data_collator([ds[i] for i in range(4)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lob001",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
