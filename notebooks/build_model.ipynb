{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from datasets import Dataset\n",
    "from transformers import BertForMaskedLM\n",
    "from theorder import LoBertModel, LoBertConfig, LoBertForMaskedLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'time_ids', 'volume_ids'],\n",
       "    num_rows: 3040\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_data_sample = \"/workspaces/2025 LoBERT/data/LOBSTER_SampleFile_AAPL_2012-06-21_10/ArrowDataset\"\n",
    "ds = Dataset.load_from_disk(path_data_sample)\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetInfo(description='', citation='', homepage='', license='', features={'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'time_ids': Sequence(feature=Value(dtype='float64', id=None), length=-1, id=None), 'volume_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}, post_processed=None, supervised_keys=None, builder_name=None, dataset_name=None, config_name=None, version=None, splits=None, download_checksums=None, download_size=None, post_processing_size=None, dataset_size=None, size_in_bytes=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoBertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 128,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 512,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"lobert\",\n",
       "  \"num_attention_heads\": 2,\n",
       "  \"num_hidden_layers\": 2,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.48.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 3200\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_lobert_tiny = {\n",
    "    \"hidden_size\": 128,\n",
    "    \"intermediate_size\": 512,\n",
    "    \"layer_norm_eps\": 1e-12,\n",
    "    \"max_position_embeddings\": 512,\n",
    "    \"model_type\": \"lobert\",\n",
    "    \"num_attention_heads\": 2,\n",
    "    \"num_hidden_layers\": 2,\n",
    "    \"vocab_size\": 3200\n",
    "}\n",
    "config = LoBertConfig(**config_lobert_tiny)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.bos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base LoBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoBERT model has 889,216 parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LoBertModel(\n",
       "  (embeddings): LoBertEmbeddings(\n",
       "    (message_embeddings): Embedding(3200, 128, padding_idx=0)\n",
       "    (time_embeddings): ValueProjectionEmbedding(\n",
       "      (value_projection): Linear(in_features=1, out_features=128, bias=True)\n",
       "    )\n",
       "    (volume_embeddings): ValueProjectionEmbedding(\n",
       "      (value_projection): Linear(in_features=1, out_features=128, bias=True)\n",
       "    )\n",
       "    (position_embeddings): Embedding(512, 128)\n",
       "    (token_type_embeddings): Embedding(2, 128)\n",
       "    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-1): 2 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = LoBertModel(config)\n",
    "print(f\"LoBERT model has {base_model.num_parameters():,.0f} parameters\")\n",
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = ds.with_format(\"torch\")[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.4504,  1.1949,  1.2456,  ..., -1.0610, -0.1199, -0.6303],\n",
       "         [ 0.7369,  0.9356,  0.7063,  ..., -0.6261,  0.2679, -0.8705],\n",
       "         [ 0.8703,  1.0277,  0.6671,  ..., -0.5746,  0.0114, -0.8967],\n",
       "         ...,\n",
       "         [ 0.1019,  0.8035, -0.0128,  ..., -1.0803, -0.0914, -0.1595],\n",
       "         [ 0.1433,  0.6966,  1.1382,  ..., -1.1918,  0.0935, -0.1655],\n",
       "         [ 0.0953,  0.5953,  1.0202,  ..., -1.2073, -0.1533, -0.0633]],\n",
       "\n",
       "        [[ 0.0118,  0.7161,  1.2969,  ..., -0.9773, -0.2765, -0.0415],\n",
       "         [ 0.3487,  0.7441,  1.2599,  ..., -0.8139,  0.0673, -0.1321],\n",
       "         [ 0.1388,  0.6827,  1.0154,  ..., -1.0839, -0.0251, -0.1003],\n",
       "         ...,\n",
       "         [ 0.6784,  1.1624,  0.9364,  ..., -0.0115, -0.0384, -0.6741],\n",
       "         [ 0.7300, -0.1894,  0.5839,  ..., -0.4427,  0.0804, -0.8113],\n",
       "         [ 0.7470,  1.0561,  0.8367,  ..., -0.7418,  0.0226, -0.7400]],\n",
       "\n",
       "        [[ 0.6761,  0.8791,  0.5708,  ..., -0.4479,  0.0962, -0.8300],\n",
       "         [ 0.7154,  0.8859,  0.6195,  ..., -0.4332,  0.0884, -0.8744],\n",
       "         [ 0.1362,  0.6004,  1.0559,  ..., -0.0809, -0.0896, -0.1165],\n",
       "         ...,\n",
       "         [ 0.7298,  1.0255, -0.0201,  ...,  0.0783,  0.1410, -0.7853],\n",
       "         [ 0.1403,  0.9510,  0.7030,  ..., -0.4058,  0.1041, -0.8198],\n",
       "         [ 0.8101,  0.9450,  0.7339,  ..., -0.5566, -0.0732, -0.8266]],\n",
       "\n",
       "        [[ 0.2201,  0.7927,  1.0871,  ..., -0.0444, -0.0386, -0.4040],\n",
       "         [ 0.6342,  1.1657, -0.0618,  ..., -1.0918,  0.0606, -0.7374],\n",
       "         [ 0.4566,  1.0420,  1.1330,  ..., -1.0974, -0.1021, -0.3148],\n",
       "         ...,\n",
       "         [ 0.6860,  0.9585,  0.5888,  ..., -0.4943,  0.0085, -0.8657],\n",
       "         [ 0.6998,  0.9391,  0.5614,  ..., -0.4274,  0.0533, -0.8381],\n",
       "         [ 0.7480,  0.9763,  0.6173,  ..., -0.4338,  0.1263, -0.8391]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.2555,  0.2603,  0.2524, -0.1663, -0.3274,  0.0376,  0.1101,  0.1583,\n",
       "         -0.1623,  0.1688, -0.0941,  0.0201, -0.1363, -0.3039, -0.1272,  0.2135,\n",
       "         -0.1320,  0.3771, -0.1572, -0.0142, -0.3680,  0.4290,  0.0706, -0.1087,\n",
       "         -0.0200,  0.1725,  0.2840,  0.0575, -0.2087,  0.2058,  0.2059,  0.0992,\n",
       "          0.4709,  0.2554, -0.1269, -0.2789,  0.2208,  0.2989,  0.1784, -0.2461,\n",
       "          0.4321, -0.1399, -0.2884,  0.3935,  0.0219,  0.0365, -0.1443,  0.1268,\n",
       "         -0.0769,  0.1723, -0.2360,  0.1285, -0.1808, -0.2114, -0.1005, -0.0175,\n",
       "         -0.1542,  0.1168, -0.2645, -0.0803, -0.1845,  0.2522,  0.1626,  0.0105,\n",
       "         -0.0411, -0.1251, -0.3133,  0.0848, -0.4118, -0.2704, -0.2042, -0.3912,\n",
       "          0.2978,  0.1312,  0.1251,  0.1669, -0.1593,  0.1281,  0.1117, -0.0393,\n",
       "          0.3312,  0.1938, -0.1571, -0.2556, -0.0874,  0.1708, -0.2817,  0.2957,\n",
       "         -0.4630, -0.5239,  0.0632,  0.2228, -0.1798, -0.1868, -0.3546, -0.1020,\n",
       "          0.1567, -0.3439,  0.1628, -0.1216,  0.3034,  0.3435, -0.2538, -0.0854,\n",
       "         -0.2650, -0.0105,  0.3318,  0.0863,  0.0971, -0.1502, -0.0031,  0.1676,\n",
       "          0.1405,  0.4277, -0.2398,  0.2464,  0.3765, -0.2988, -0.1796,  0.0178,\n",
       "          0.0513,  0.1337, -0.0216,  0.1446, -0.1574,  0.0277, -0.1138,  0.1979],\n",
       "        [-0.0015,  0.3060,  0.3379,  0.0127,  0.0229, -0.0011,  0.1667,  0.2171,\n",
       "         -0.2817, -0.0567, -0.2385,  0.0281,  0.0441, -0.4077, -0.0590,  0.0630,\n",
       "          0.2334,  0.2762,  0.0194, -0.0276, -0.5175,  0.2891, -0.3301, -0.1271,\n",
       "          0.0335, -0.0118,  0.2719, -0.0367,  0.0317,  0.0119,  0.4440,  0.2007,\n",
       "          0.1077,  0.1233, -0.2469,  0.0245,  0.4299,  0.3561,  0.0514, -0.3257,\n",
       "          0.1161, -0.0956, -0.0595,  0.1799, -0.0507,  0.0854, -0.3485,  0.0136,\n",
       "         -0.1494,  0.1313, -0.3473, -0.1547,  0.1642,  0.0782, -0.1952,  0.1632,\n",
       "         -0.2702, -0.1628, -0.3566,  0.0455, -0.2353, -0.1259,  0.3005, -0.2002,\n",
       "          0.0047,  0.0667, -0.2962, -0.1559, -0.2120,  0.2612, -0.0644, -0.3426,\n",
       "         -0.0405,  0.0698,  0.1302,  0.0555, -0.1622,  0.1518, -0.2638, -0.2525,\n",
       "          0.1756,  0.1565, -0.0234,  0.1654, -0.1599, -0.0731, -0.2512,  0.1348,\n",
       "         -0.1163, -0.4533,  0.1546, -0.0341, -0.0369, -0.2665, -0.0664,  0.0517,\n",
       "          0.2477, -0.1940, -0.0485,  0.1063,  0.5224,  0.1833, -0.1979, -0.3333,\n",
       "         -0.3151,  0.2467,  0.2797,  0.1264,  0.0174, -0.0859, -0.1524, -0.0891,\n",
       "         -0.1119,  0.3627, -0.1965,  0.2014,  0.1888, -0.1947, -0.3171,  0.0456,\n",
       "         -0.0054,  0.1468,  0.2325,  0.1652, -0.0997, -0.0935, -0.0171, -0.0350],\n",
       "        [-0.1609, -0.0807,  0.0514, -0.1479, -0.3318,  0.1590,  0.1618, -0.2179,\n",
       "          0.0781,  0.2284,  0.1636,  0.0813, -0.2221, -0.2616, -0.0402,  0.2798,\n",
       "         -0.5294,  0.4124,  0.0089, -0.0762,  0.0163,  0.1471,  0.0069, -0.0867,\n",
       "          0.0089,  0.2215,  0.3348, -0.0482, -0.1924,  0.2550,  0.0315, -0.1295,\n",
       "          0.3322,  0.1784, -0.2731, -0.4357, -0.2152, -0.0205,  0.0065, -0.1513,\n",
       "          0.5318, -0.2897, -0.4127,  0.3874,  0.0737,  0.0059,  0.0390,  0.1106,\n",
       "         -0.0691, -0.0304, -0.2665,  0.2075, -0.0177, -0.3542, -0.1095, -0.2744,\n",
       "          0.0438,  0.2946, -0.0539, -0.0746, -0.0617,  0.1460, -0.0099,  0.0038,\n",
       "         -0.0794, -0.2632, -0.2213,  0.1154, -0.3107, -0.4022, -0.2191, -0.0966,\n",
       "          0.5170, -0.0223,  0.2940,  0.2167, -0.1843,  0.2762,  0.4988,  0.2235,\n",
       "          0.2343,  0.1397, -0.0487, -0.3480,  0.1063,  0.3279, -0.2065,  0.1432,\n",
       "         -0.5994, -0.1740,  0.0145,  0.4660, -0.1729, -0.0039, -0.3578, -0.2681,\n",
       "          0.0899, -0.2513,  0.0579,  0.1114, -0.0907,  0.2855, -0.2192,  0.1098,\n",
       "         -0.3989, -0.1413,  0.2488,  0.1389,  0.2330, -0.0885,  0.0936,  0.1500,\n",
       "          0.2706,  0.2365, -0.3636,  0.3163,  0.4410, -0.1763,  0.0206, -0.0090,\n",
       "          0.1621, -0.1596, -0.1169, -0.0871, -0.1629,  0.1339, -0.1585,  0.2680],\n",
       "        [ 0.0689,  0.3578,  0.2145,  0.0164, -0.0927, -0.0455,  0.3934,  0.0864,\n",
       "         -0.1628, -0.0469, -0.2060,  0.0086, -0.0136, -0.3556,  0.0403,  0.2403,\n",
       "          0.0471,  0.3718,  0.0987,  0.0628, -0.5258,  0.3266, -0.2411, -0.1356,\n",
       "         -0.1166, -0.0589,  0.3340,  0.1085,  0.0490,  0.0111,  0.3755,  0.1815,\n",
       "          0.1522,  0.1779, -0.3491, -0.1576,  0.3532,  0.3109, -0.0443, -0.3245,\n",
       "          0.3195, -0.1082, -0.0907,  0.3677, -0.1017,  0.1794, -0.3373,  0.0985,\n",
       "         -0.0472,  0.1912, -0.3614, -0.1612,  0.2099,  0.1148, -0.3441,  0.1461,\n",
       "         -0.1776, -0.2251, -0.3719,  0.0556, -0.3149, -0.2687,  0.2806, -0.1750,\n",
       "         -0.0416, -0.1167, -0.2661, -0.0539, -0.2091,  0.1257, -0.1702, -0.3789,\n",
       "          0.1317,  0.0513,  0.3144,  0.1711, -0.1809,  0.2007, -0.0934, -0.0493,\n",
       "          0.1755,  0.2127,  0.1493,  0.0723, -0.1268,  0.0304, -0.3046,  0.1278,\n",
       "         -0.2135, -0.4359,  0.0259,  0.1063, -0.1691, -0.2133, -0.2281,  0.0261,\n",
       "          0.3109, -0.2464, -0.0612,  0.0909,  0.4336,  0.1321, -0.3080, -0.3384,\n",
       "         -0.3270,  0.2221,  0.4234,  0.0846,  0.1237, -0.1473, -0.3252, -0.0572,\n",
       "         -0.1287,  0.4068, -0.2139,  0.1719,  0.3127, -0.2106, -0.2633,  0.1869,\n",
       "         -0.1268,  0.1951,  0.1150,  0.0226, -0.1330, -0.0422, -0.0680, -0.0258]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model(**input_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked message modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LoBertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LoBertForMaskedLM(\n",
       "  (bert): LoBertModel(\n",
       "    (embeddings): LoBertEmbeddings(\n",
       "      (message_embeddings): Embedding(3200, 128, padding_idx=0)\n",
       "      (time_embeddings): ValueProjectionEmbedding(\n",
       "        (value_projection): Linear(in_features=1, out_features=128, bias=True)\n",
       "      )\n",
       "      (volume_embeddings): ValueProjectionEmbedding(\n",
       "        (value_projection): Linear(in_features=1, out_features=128, bias=True)\n",
       "      )\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-1): 2 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): LoBertOnlyMLMHead(\n",
       "    (predictions): LoBertLMPredictionHead(\n",
       "      (transform): LoBertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=128, out_features=3200, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmmodel = LoBertForMaskedLM(config)\n",
    "mmmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of theorder failed: Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/lob001/lib/python3.11/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/opt/conda/envs/lob001/lib/python3.11/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/lob001/lib/python3.11/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 621, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/workspaces/2025 LoBERT/src/package/theorder/__init__.py\", line 4, in <module>\n",
      "    from .data import DataCollatorForMessageModeling\n",
      "  File \"/workspaces/2025 LoBERT/src/package/theorder/data/__init__.py\", line 1, in <module>\n",
      "    from .data_collator import DataCollatorForMessageModeling\n",
      "  File \"/workspaces/2025 LoBERT/src/package/theorder/data/data_collator.py\", line 14, in <module>\n",
      "    class DataCollatorForMessageModeling(DataCollatorMixin):\n",
      "  File \"/workspaces/2025 LoBERT/src/package/theorder/data/data_collator.py\", line 44, in DataCollatorForMessageModeling\n",
      "    tokenizer: PreTrainedTokenizerBase\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "NameError: name 'PreTrainedTokenizerBase' is not defined\n",
      "]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DataCollatorForLanguageModeling' from 'theorder' (/workspaces/2025 LoBERT/src/package/theorder/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtheorder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataCollatorForLanguageModeling\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'DataCollatorForLanguageModeling' from 'theorder' (/workspaces/2025 LoBERT/src/package/theorder/__init__.py)"
     ]
    }
   ],
   "source": [
    "from theorder import DataCollatorForLanguageModeling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lob001",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
